import csv
import numpy as np
import pandas as pd
import re
import spacy
import string
import textstat as ts

from cainesap_syllabify import syllable3
from nltk import pos_tag
from nltk.stem import WordNetLemmatizer
from nltk.tokenize import sent_tokenize, word_tokenize
from os import listdir, makedirs
from os.path import isfile, join, splitext
from punctuator import Punctuator

lemmatizer = WordNetLemmatizer()
nlp = spacy.load('en_core_web_sm')
p = Punctuator('Demo-Europarl-EN.pcl')

wordlists_dir = "local/resources/wordlists"
dale_list_path = join(wordlists_dir, "dale-chall-detailed.csv")
dale_df = pd.read_csv(dale_list_path, header=0)

system_map = dict({
    'actual': 'actual',
    'gws': 'google-web-speech',
    'ka5': 'kaldi-aspire-s5'
})

def average_sentence_length(text):
    ave_length = 0.0

    sent_count = ts.sentence_count(text)
    word_count = ts.lexicon_count(text)

    ave_length = word_count / sent_count

    return ave_length

def average_word_length(text):
    ave_length = 0.0

    syll_count = count_syllables_cainesap(text)
    word_count = ts.lexicon_count(text)

    ave_length = syll_count / word_count

    return ave_length

def compute_dcr(text, words_in_sample = 100, sample_count = 3):
    """
    This function computes the Dale-Chall Readability (DCR) score based from 
    the 1948 paper by E. Dale and J. Chall, "A Formula for Predicting 
    Readability: Instructions".

    Parameters
    ----------
    text : string
        Transcript to be processed
    words_in_sample : integer
        Number of words for each sample from the text, default is 100 words
    sample_count : integer
        Number of samples to be extracted from long texts, default is 3 samples

    Returns
    -------
    score : float
        DCR score
        For long texts, this is the average readability score from all the
        samples

    """
    score = 0.0
    text_length = ts.lexicon_count(text)
    # Dale-Chall suggests the following for getting samples from long texts
    # * About four 100-word samples per 2,000 words.
    # * For passages of about 200 to 300 words, analyse the entire passage
    # From this, we made the following assumptions
    # * long texts = word counts > 300
    # * lower limit of 3 samples for long texts
    # * beginnings of a paragraph may not always be available (e.g., transcript
    # generated by an ASR)
    # * samples are not exactly composed of 100 words (considering end of
    # sentences) and might be over 100
    # * minimize, if not avoid, overlaps among samples
    word_low_limit = 300

    if text_length <= word_low_limit:
        # Text is too short, use the entire text
        score = dale_chall_readability_raw(text)
        scores = np.empty((1,sample_count))
        scores[:] = np.nan
    else:
        scores = []

        # Ensure equal spacing among samples
        target_starts = np.arange(0, sample_count) * np.floor(
            text_length / sample_count
        )

        # Split text into sentences, determine lengths and index of starting
        # words
        sentences = re.findall(r'\b[^.!?]+[.!?]*', text, re.UNICODE)
        sentence_lengths = [
            ts.lexicon_count(sentence) for sentence in sentences
        ]
        sentence_starts = np.insert(np.cumsum(sentence_lengths), 0, 0)

        # Get text samples and compute the readability scores
        for target_start in target_starts:
            # Get index of starting sentence
            start_dist = sentence_starts - target_start
            index_start = np.where(
                start_dist >= 0, start_dist, np.inf
            ).argmin()

            # Get index of ending sentence
            end_dist = start_dist - (words_in_sample + 1)
            index_end = np.abs(end_dist).argmin()
            if index_end == index_start:
                index_end += 1

            if index_end > len(sentences):
                sample = " ".join(sentences[index_start:])
            else:
                sample = " ".join(sentences[index_start:index_end])

            # Compute the score for the text sample
            scores.append(dale_chall_readability_raw(sample))

        # Get the average
        score = sum(scores) / len(scores)
        scores = np.array(scores)

    return round(score, 4),scores

def compute_fkgl(text, words_in_sample = 100, sample_count = 3):
    """
    This function calculates the Flesch-Kincaid Grade Level (FKGL) based
    from the 1975 paper by J. Kincaid et. al., "Derivation Of New Readability
    Formulas (Automated Readability Index, Fog Count And Flesch Reading Ease
    Formula) For Navy Enlisted Personnel".

    Parameters
    ----------
    text : string
        Transcript to be processed
    words_in_sample : integer
        Number of words for each sample from the text, default is 100 words
    sample_count : integer
        Number of samples to be extracted from long texts, default is 3 samples

    Returns
    -------
    score : float
        FRE formula score
        For long texts, this is the average readability score from all the
        samples

    """
    score = 0.0
    text_length = ts.lexicon_count(text)
    # There are no specific instructions for dealing with long texts, so we
    # adopt the same assumptions used in the calculation of the other metrics.
    word_low_limit = 300

    if text_length <= word_low_limit:
        # Text is too short, use the entire text
        score = flesch_kincaid_grade_level(text)
        scores = np.empty((1,sample_count))
        scores[:] = np.nan
    else:
        scores = []

        # Ensure equal spacing among samples
        target_starts = np.arange(0, sample_count) * np.floor(
            text_length / sample_count
        )

        # Split text into sentences, determine lengths and index of starting
        # words
        sentences = re.findall(r'\b[^.!?]+[.!?]*', text, re.UNICODE)
        sentence_lengths = [
            ts.lexicon_count(sentence) for sentence in sentences
        ]
        sentence_starts = np.insert(np.cumsum(sentence_lengths), 0, 0)

        # Get text samples and compute the readability scores
        for target_start in target_starts:
            # Get index of starting sentence
            start_dist = sentence_starts - target_start
            index_start = np.where(
                start_dist >= 0, start_dist, np.inf
            ).argmin()

            # Get index of ending sentence
            end_dist = start_dist - (words_in_sample + 1)
            index_end = np.abs(end_dist).argmin()
            if index_end == index_start:
                index_end += 1

            if index_end > len(sentences):
                sample = " ".join(sentences[index_start:])
            else:
                sample = " ".join(sentences[index_start:index_end])

            # Compute the score for the text sample
            scores.append(flesch_kincaid_grade_level(sample))

        # Get the average
        score = sum(scores) / len(scores)
        scores = np.array(scores)

    return round(score, 4),scores

def compute_fre(text, words_in_sample = 100, sample_count = 3):
    """
    This function calculates the Flesch Reading Ease (FRE) formula score based
    from the 1949 paper by R. Flesch, "A New Readability Yardstick".

    Parameters
    ----------
    text : string
        Transcript to be processed
    words_in_sample : integer
        Number of words for each sample from the text, default is 100 words
    sample_count : integer
        Number of samples to be extracted from long texts, default is 3 samples

    Returns
    -------
    score : float
        FRE formula score
        For long texts, this is the average readability score from all the
        samples

    """
    score = 0.0
    text_length = ts.lexicon_count(text)
    # Flesch suggests using 3 to 5 100-word samples of an article for
    # computing the readability score, and sample must start at the beginning
    # of a paragraph.
    # From this we made the following assumptions
    # * long texts = word counts > 300
    # * lower limit of 3 samples for long texts
    # * beginnings of a paragraph may not always be available (e.g., transcript
    # generated by an ASR)
    # * samples are not exactly composed of 100 words (considering end of
    # sentences) and might be over 100
    # * minimize, if not avoid, overlaps among samples
    word_low_limit = 300


    if text_length <= word_low_limit:
        # Text is too short, use the entire text
        score = flesch_reading_ease(text)
        scores = np.empty((1,sample_count))
        scores[:] = np.nan
    else:
        scores = []

        # Ensure equal spacing among samples
        target_starts = np.arange(0, sample_count) * np.floor(
            text_length / sample_count
        )

        # Split text into sentences, determine lengths and index of starting
        # words
        sentences = re.findall(r'\b[^.!?]+[.!?]*', text, re.UNICODE)
        sentence_lengths = [
            ts.lexicon_count(sentence) for sentence in sentences
        ]
        sentence_starts = np.insert(np.cumsum(sentence_lengths), 0, 0)

        # Get text samples and compute the readability scores
        for target_start in target_starts:
            # Get index of starting sentence
            start_dist = sentence_starts - target_start
            index_start = np.where(
                start_dist >= 0, start_dist, np.inf
            ).argmin()

            # Get index of ending sentence
            end_dist = start_dist - (words_in_sample + 1)
            index_end = np.abs(end_dist).argmin()
            if index_end == index_start:
                index_end += 1

            if index_end > len(sentences):
                sample = " ".join(sentences[index_start:])
            else:
                sample = " ".join(sentences[index_start:index_end])

            # Compute the score for the text sample
            scores.append(flesch_reading_ease(sample))

        # Get the average
        score = sum(scores) / len(scores)
        scores = np.array(scores)

    return round(score, 4),scores

def compute_from_dir(
    texts_dir, 
    score_writer,
    is_punct = False,
    is_limit = False,
    count_limit = 100,
    is_save = True
):
    """
    This function calculates readability scores of all transcripts in a
    directory. It assumes that one file contains one transcript only.

    Parameters
    ----------
    texts_dir : string
        Directory where all the transcripts are stored
    score_writer : class '_csv.writer'
        Tells where to save the readability scores for each transcript
    is_punct : boolean
        Option to use ottokart's punctuator2,
        (https://github.com/ottokart/punctuator2)
        True if the punctuator will be used
    is_limit : boolean
        Option to truncate the text based on word count. True if the truncation
        will be applied
    count_limit : int
        Target word count of the truncated text. Default is 100 words
    is_save : boolean
        Option to keep complete sentences when a word count limit is imposed.
        This means that if the limit cuts the text in the middle of the
        sentence and this option is True, then the entire sentence will still
        be included in the truncated text instead.

    Returns
    -------
    None

    """

    for entry in sorted(listdir(texts_dir)):
        text_path = join(texts_dir, entry)
        utt_id = splitext(entry)[0]

        if isfile(text_path) and entry.endswith(".txt"):
            print("Analyzing %s" % entry)
            text_f = open(text_path, mode = 'r')
            text = text_f.read()
            text_f.close()

            text = text.replace('\n', ' ').strip()

            scores = compute_scores(
                text, 
                is_punct = is_punct,
                is_limit = is_limit,
                count_limit = count_limit,
                is_save = is_save
            )

            scores.insert(0, utt_id)
            score_writer.writerow(scores)

def compute_from_list(
    transcript_file, 
    score_writer,
    is_punct = False,
    is_limit = False,
    count_limit = 100,
    is_save = True
):
    """
    Calculate readability scores of all files in a directory

    Parameters
    ----------
    transcript_file : string
        Text file containing all the transcripts. One line contains one
        transcript. Format is <utt_id> <transcript>
    score_writer : class '_csv.writer'
        Tells where to save the readability scores for each transcript
    is_punct : boolean
        Option to use ottokart's punctuator2,
        (https://github.com/ottokart/punctuator2)
        True if the punctuator will be used
    is_limit : boolean
        Option to truncate the text based on word count True if the truncation
        will be applied
    count_limit : int
        Target word count of the truncated text. Default is 100 words
    is_save : boolean
        Option to keep complete sentences when a word count limit is imposed.
        This means that if the limit cuts the text in the middle of the
        sentence and this option is True, then the entire sentence will still
        be included in the truncated text instead.

    Returns
    -------
    None

    """

    with open(transcript_file, 'r') as tr_in_f:
        for entry in tr_in_f:
            utt_id,text = entry.strip().split(" ", maxsplit=1)
            print("Analyzing %s" % utt_id)

            text = re.sub(r'\[NOISE\]', "", text)
            text = re.sub(r' +', " ", text)

            scores = compute_scores(
                text, 
                is_punct = is_punct,
                is_limit = is_limit,
                count_limit = count_limit,
                is_save = is_save
            )

            scores.insert(0, utt_id)
            score_writer.writerow(scores)

def compute_lw(text, words_in_sample = 100, sample_count = 3):
    """
    This function computes the Lensear Write (LW) formula score based from the
    1966 paper by J. O'Hayre, "Gobbledygook Has Gotta Go", using several
    samples for long texts.

    Parameters
    ----------
    text : string
        Transcript to be processed
    words_in_sample : integer
        Number of words for each sample from the text, default is 100 words
    sample_count : integer
        Number of samples to be extracted from long texts, default is 3 samples

    Returns
    -------
    score : float
        LW formula score
        For long texts, this is the average readability score from all the
        samples

    """
    score = 0.0
    text_length = ts.lexicon_count(text)
    # From this we made the following assumptions
    # * long texts = word counts > 300
    # * lower limit of 3 samples for long texts
    # * beginnings of a paragraph may not always be available (e.g., transcript
    # generated by an ASR)
    # * samples are not exactly composed of 100 words (considering end of
    # sentences) and might be over 100
    # * minimize, if not avoid, overlaps among samples
    word_low_limit = 300

    # Split text into sentences, determine lengths and index of starting
    # words
    sentences = re.findall(r'\b[^.!?]+[.!?]*', text, re.UNICODE)
    sentence_lengths = [ts.lexicon_count(sentence) for sentence in sentences]
    sentence_starts = np.insert(np.cumsum(sentence_lengths), 0, 0)

    if text_length <= word_low_limit:
        # Text is too short, get one sample only
        index_start = 0

        # Get index of ending sentence
        end_dist = sentence_starts - (words_in_sample + 1)
        index_end = np.abs(end_dist).argmin()
        if index_end == index_start:
            index_end += 1

        if index_end > len(sentences):
            sample = " ".join(sentences[index_start:])
        else:
            sample = " ".join(sentences[index_start:index_end])

        score = lensear_write(text)
        scores = np.empty((1,sample_count))
        scores[:] = np.nan
    else:
        scores = []

        # Ensure equal spacing among samples
        target_starts = np.arange(0, sample_count) * np.floor(
            text_length / sample_count
        )

        # Get text samples and compute the readability scores
        for target_start in target_starts:
            # Get index of starting sentence
            start_dist = sentence_starts - target_start
            index_start = np.where(
                start_dist >= 0, start_dist, np.inf
            ).argmin()

            # Get index of ending sentence
            end_dist = start_dist - (words_in_sample + 1)
            index_end = np.abs(end_dist).argmin()
            if index_end == index_start:
                index_end += 1

            if index_end > len(sentences):
                sample = " ".join(sentences[index_start:])
            else:
                sample = " ".join(sentences[index_start:index_end])

            # Compute the score for the text sample
            scores.append(lensear_write(sample))

        # Get the average
        score = sum(scores) / len(scores)
        scores = np.array(scores)

    return round(score, 4),scores

def compute_mer(text, words_in_sample = 100, sample_count = 3):
    """
    This function calculates the McAlpine EFLAW readability (MER) score based
    from several published works using the metric.

    Parameters
    ----------
    text : string
        Transcript to be processed
    words_in_sample : integer
        Number of words for each sample from the text, default is 100 words
    sample_count : integer
        Number of samples to be extracted from long texts, default is 3 samples

    Returns
    -------
    score : float
        MER score
        For long texts, this is the average readability score from all the
        samples

    """
    score = 0.0
    text_length = ts.lexicon_count(text)
    # There are no specific instructions for dealing with long texts, so we
    # adopt the same assumptions used in the calculation of the other metrics.
    word_low_limit = 300


    if text_length <= word_low_limit:
        # Text is too short, use the entire text
        score = ts.mcalpine_eflaw(text)
        scores = np.empty((1,sample_count))
        scores[:] = np.nan
    else:
        scores = []

        # Ensure equal spacing among samples
        target_starts = np.arange(0, sample_count) * np.floor(
            text_length / sample_count
        )

        # Split text into sentences, determine lengths and index of starting
        # words
        sentences = re.findall(r'\b[^.!?]+[.!?]*', text, re.UNICODE)
        sentence_lengths = [
            ts.lexicon_count(sentence) for sentence in sentences
        ]
        sentence_starts = np.insert(np.cumsum(sentence_lengths), 0, 0)

        # Get text samples and compute the readability scores
        for target_start in target_starts:
            # Get index of starting sentence
            start_dist = sentence_starts - target_start
            index_start = np.where(
                start_dist >= 0, start_dist, np.inf
            ).argmin()

            # Get index of ending sentence
            end_dist = start_dist - (words_in_sample + 1)
            index_end = np.abs(end_dist).argmin()
            if index_end == index_start:
                index_end += 1

            if index_end > len(sentences):
                sample = " ".join(sentences[index_start:])
            else:
                sample = " ".join(sentences[index_start:index_end])

            # Compute the score for the text sample
            scores.append(ts.mcalpine_eflaw(sample))

        # Get the average
        score = sum(scores) / len(scores)
        scores = np.array(scores)

    return round(score, 4),scores

def compute_scores(
    text, is_punct = False, is_limit = False, count_limit = 100, is_save = True
):
    """
    Calculate readability scores of all files in a directory

    Parameters
    ----------
    text : string
        Transcript to be processed
    is_punct : boolean
        Option to use ottokart's punctuator2,
        (https://github.com/ottokart/punctuator2)
        True if the punctuator will be used
    is_limit : boolean
        Option to truncate the text based on word count True if the truncation
        will be applied
    count_limit : int
        Target word count of the truncated text. Default is 100 words
    is_save : boolean
        Option to keep complete sentences when a word count limit is imposed.
        This means that if the limit cuts the text in the middle of the
        sentence and this option is True, then the entire sentence will still
        be included in the truncated text instead.
    Returns
    -------
    scores : list
        Readability scores
    """

    if is_punct:
        text = remove_punctuation(text)
        text = p.punctuate(text.lower())

    if is_limit:
        text = limit_text_by_word_count(
            text, count_limit = count_limit, is_save = is_save
        )

    all_dcr = dale_chall_readability_raw(text)
    all_fkgl = flesch_kincaid_grade_level(text)
    all_fre = flesch_reading_ease(text)
    all_lw = lensear_write(text)
    all_mer = ts.mcalpine_eflaw(text)
    udcr,samp_dcr = compute_dcr(text)
    ufkgl,samp_fkgl = compute_fkgl(text)
    ufre,samp_fre = compute_fre(text)
    ulw,samp_lw = compute_lw(text)
    umer,samp_mer = compute_mer(text)
    scores = [
        all_dcr, all_fkgl, all_fre, all_lw, all_mer, 
        udcr, ufkgl, ufre, ulw, umer, 
        samp_dcr, samp_fkgl, samp_fre, samp_lw, samp_mer
    ]

    return scores

def count_in_dale_list(text):
    count = 0.0

    sentences = sent_tokenize(text)

    for sentence in sentences:
        for word, word_pos in pos_tag(word_tokenize(sentence)):
            if (
                word and 
                (word not in string.punctuation) and 
                is_in_dale_list(word, word_pos)
            ):
                count += 1

    return count

def count_syllables_cainesap(text):
    count = 0
    text = remove_punctuation(text)

    for word in text.split():
        syllables = syllable3.generate(word)
        if syllables:
            try:
                count += len(list(syllables)[0])
            except:
                # print("WARNING: cannot process %s. Defaults to pyphen." \
                #       % (word.upper()))
                count += ts.syllable_count(word)
                pass
    return count

def dale_chall_readability_raw(text):
    score = 0.0

    word_count = ts.lexicon_count(text)
    not_in_dale_count = word_count - count_in_dale_list(text)

    ave_sent_len = average_sentence_length(text)
    dale_score = not_in_dale_count * 100 / word_count

    score = (0.0496 * ave_sent_len) + (0.1579 * dale_score)
    
    if dale_score > 0.05:
        score = score + 3.6365

    return score

def flesch_kincaid_grade_level(text):
    score = 0.0

    ave_sent_len = average_sentence_length(text)
    ave_word_len = average_word_length(text)

    score = (0.39 * ave_sent_len) + (11.8 * ave_word_len) - 15.59

    return score

def flesch_reading_ease(text):
    score = 0.0

    ave_sent_len = average_sentence_length(text)
    ave_word_len = average_word_length(text)

    score = 206.835 - (84.6 * ave_word_len) - (1.015 * ave_sent_len)

    return score

def is_in_dale_list(word, word_pos):
    bool_in_dale = False 

    if (dale_df['word'].eq(word.lower())).any():
        bool_in_dale = True
    elif word.isnumeric():
        # number
        bool_in_dale = True
    elif word.endswith("en"):
        # misc: word ending in "en"
        bool_in_dale = False
    elif word_pos == "NNP":
        # noun
        check_ner = [ent.label_ for ent in nlp(word).ents]
        if check_ner:
            word_ner = check_ner[0]
            if word_ner and word_ner in ["PERSON", "GPE"]:
                bool_in_dale = True
    elif word_pos in ["VB", "VBD", "VBG", "VBN", "VBP", "VBZ"]:
        # verb
        word_lemma = lemmatizer.lemmatize(word.lower(), pos='v')
        if (dale_df['lemma_v'].eq(word_lemma)).any():
            bool_in_dale = True
    elif word_pos in ["JJ", "JJR", "JJS"]:
        # adjective
        if (dale_df['lemma_a'].eq(
            re.sub(r'e[rs]t?$', "", word.lower()))).any():
            # comparative or superlative
            bool_in_dale = True
        elif word.endswith("n"):
            word_lemma = word[:-1]
            word_lemma_pos = pos_tag([word_lemma])[0][1]
            if word_lemma_pos in ["NN", "NNP", "NNS"]:
                # formed from a noun
                bool_in_dale = True
    elif word_pos == "RB":
        # adverb
        if (dale_df['word'].eq(re.sub(r'ly$','',word.lower()))).any():
            bool_in_dale = True
    elif "-" in word:
        # hypenated word
        if all(
            (dale_df['word'].eq(w)).any() 
            for w in re.sub("-", " ", word.lower()).split()
        ):
            bool_in_dale = True
    
    return bool_in_dale

def lensear_write(text):
    """
    This function computes the Lensear Write (LW) formula score based from the
    1966 paper by J. O'Hayre, "Gobbledygook Has Gotta Go", for one sample.

    Parameters
    ----------
    text : string
        Transcript to be processed
    Returns
    -------
    score : float
        LW formula score

    """
    # O'Hayre suggests using a 100-word sample for computing the readability
    # score. The score is computed as follows
    # 1. Count a 100-word sample.
    # 2. Count all one-syllable words except ''the", "is", "are", "was", and
    # "were". Count one point for each one-syllable word.
    # 3. Count the number of sentences in the 100-word sample to the nearest
    # period or semicolon and give three points for each sentence.
    # 4. Add together the one-syllable word count and the three points for each
    # sentence to get your grade.
    # 5. If your piece has less than 100 words, multiply your tally to get the
    # equivalent of 100
    ignore_words = ["the", "is", "are", "was", "were"]
    monosyl_count = 0
    score = 0.0
    word_count = ts.lexicon_count(text)

    text = remove_punctuation(text)

    if word_count > 100:
        text = " ".join(text.split()[0:100])

    sent_count = ts.sentence_count(text)
    word_count = ts.lexicon_count(text)
    
    for word in text.split():
        if word.lower() not in ignore_words:
            if count_syllables_cainesap(word) == 1:
                monosyl_count +=1

    score = monosyl_count + (3.0 * sent_count)
    if word_count < 100:
        score = score * (100.0 / word_count)

    return score

def limit_text_by_word_count(text, count_limit = 100, is_save = True):
    """
    Truncate text based on a word count limit

    Parameters
    ----------
    text : string
        Transcript to be processed
    count_limit : int
        Target word count of the truncated text. Default is 100 words
    is_save : boolean
        Option to keep complete sentences when a word count limit is imposed.
        This means that if the limit cuts the text in the middle of the
        sentence and this option is True, then the entire sentence will still
        be included in the truncated text instead.

    Returns
    -------
    limited_text : string
        Truncated text
    """

    if is_save:
        current_count = 0
        limited_text = ""

        sentences = re.findall(r'\b[^.!?]+[.!?]*', text, re.UNICODE)

        for sentence in sentences:
            limited_text = limited_text + " " + sentence
            current_count = ts.lexicon_count(limited_text)

            if current_count >= count_limit:
                break
    else:
        limited_text = " ".join(text.split()[:count_limit])

    return limited_text

def remove_punctuation(text):
    """
    Remove punctuations from text

    Parameters
    ----------
    text : string
        Transcript to be processed

    Returns
    -------
    text : string
        Cleaned version of the text
    """
    punctuation_regex = r"[^\w\s\']"

    text = re.sub(r"\'(?![tsd]\b|ve\b|ll\b|re\b|m\b)", '"', text)
    text = re.sub(punctuation_regex, " ", text)
    text = " ".join(text.split())

    return text

def run_voa_example():
    is_split = True
    is_punct = True
    is_sample = True
    is_limit = False
    count_limit = 100
    is_save = True

    data_name = "voa"
    sys_name = "ka5"
    header = [
        "id", "all_dcr", "all_fkgl", "all_fre", "all_lw", "all_mer", 
        "udcr", "ufkgl", "ufre", "ulw", "umer", 
        "samp_dcr", "samp_fkgl", "samp_fre", "samp_lw", "samp_mer"
    ]

    transcripts_map = {
        'gws': "voa-1000_google-web_T-chunks_T-period_hyp.txt",
        'ka5': "voa-1000_kaldi-aspire_Tr-chunks_hyp.txt"
    }

    main_dir = join("data", data_name)

    if sys_name in ["actual"]:
        texts_dir = join(main_dir, "processed/transcripts")
    elif sys_name in ["ka5", "gws"]:
        transcript_dir = join(
            main_dir, join("processed/asr",system_map[sys_name])
        )
        transcript_file = transcripts_map[sys_name]
        transcript_path = join(transcript_dir, transcript_file)
    
    results_dir = join(
        main_dir, join("results/readability",system_map[sys_name])
    )
    makedirs(results_dir, exist_ok = True)

    scores_file = f"readability_{data_name}_{sys_name}_{str(is_split)[:1]}-chunks_{str(is_punct)[:1]}-punct_{str(is_sample)[:1]}-samp_{str(is_limit)[:1]}-limit_{str(is_save)[:1]}-save.csv"
    scores_path = join(results_dir, scores_file)

    with open(scores_path, mode = 'w') as scores_f:
        score_writer = csv.writer(scores_f, delimiter=',')
        score_writer.writerow(header)

        if sys_name in ["actual"]:
            compute_from_dir(
                texts_dir, 
                score_writer, 
                is_punct = is_punct, 
                is_limit = is_limit, 
                count_limit = count_limit, 
                is_save = is_save
            )
        elif sys_name in ["gws", "ka5"]:
            compute_from_list( 
                transcript_path, 
                score_writer, 
                is_punct = is_punct, 
                is_limit = is_limit, 
                count_limit = count_limit, 
                is_save = is_save
            )
        else:
            print("Please specify valid system name")

def main():
    run_voa_example()

if __name__ == "__main__":
    main()
